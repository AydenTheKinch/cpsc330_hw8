{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning\n",
    "\n",
    "## Homework 8: Introduction to Computer vision and Time Series (Lectures 19 and 20) \n",
    "\n",
    "**Due date: see the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset and storing it under the `data` folder. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "rubric={points:4}\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture demo, we had different measurements for each Location. \n",
    "\n",
    "We want you to consider this for the avocado prices dataset. For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18249 entries, 0 to 11\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Date          18249 non-null  datetime64[ns]\n",
      " 1   AveragePrice  18249 non-null  float64       \n",
      " 2   Total Volume  18249 non-null  float64       \n",
      " 3   4046          18249 non-null  float64       \n",
      " 4   4225          18249 non-null  float64       \n",
      " 5   4770          18249 non-null  float64       \n",
      " 6   Total Bags    18249 non-null  float64       \n",
      " 7   Small Bags    18249 non-null  float64       \n",
      " 8   Large Bags    18249 non-null  float64       \n",
      " 9   XLarge Bags   18249 non-null  float64       \n",
      " 10  type          18249 non-null  object        \n",
      " 11  year          18249 non-null  int64         \n",
      " 12  region        18249 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>0.84</td>\n",
       "      <td>965185.06</td>\n",
       "      <td>438526.12</td>\n",
       "      <td>199585.90</td>\n",
       "      <td>11017.42</td>\n",
       "      <td>316055.62</td>\n",
       "      <td>153009.89</td>\n",
       "      <td>160999.10</td>\n",
       "      <td>2046.63</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7667064.46</td>\n",
       "      <td>2567279.74</td>\n",
       "      <td>1912986.38</td>\n",
       "      <td>118289.91</td>\n",
       "      <td>3068508.43</td>\n",
       "      <td>1309580.19</td>\n",
       "      <td>1745630.06</td>\n",
       "      <td>13298.18</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>271723.08</td>\n",
       "      <td>26996.28</td>\n",
       "      <td>77861.39</td>\n",
       "      <td>117.56</td>\n",
       "      <td>166747.85</td>\n",
       "      <td>87108.00</td>\n",
       "      <td>79495.39</td>\n",
       "      <td>144.46</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.03</td>\n",
       "      <td>43409835.75</td>\n",
       "      <td>14130799.10</td>\n",
       "      <td>12125711.42</td>\n",
       "      <td>758801.12</td>\n",
       "      <td>16394524.11</td>\n",
       "      <td>12540327.19</td>\n",
       "      <td>3544729.39</td>\n",
       "      <td>309467.53</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>TotalUS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume         4046         4225       4770  \\\n",
       "0 2018-03-25          0.84     965185.06    438526.12    199585.90   11017.42   \n",
       "0 2018-03-25          1.62      15303.40      2325.30      2171.66       0.00   \n",
       "0 2018-03-25          0.93    7667064.46   2567279.74   1912986.38  118289.91   \n",
       "0 2018-03-25          1.60     271723.08     26996.28     77861.39     117.56   \n",
       "0 2018-03-25          1.03   43409835.75  14130799.10  12125711.42  758801.12   \n",
       "\n",
       "    Total Bags   Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0    316055.62    153009.89   160999.10      2046.63  conventional  2018   \n",
       "0     10806.44     10569.80      236.64         0.00       organic  2018   \n",
       "0   3068508.43   1309580.19  1745630.06     13298.18  conventional  2018   \n",
       "0    166747.85     87108.00    79495.39       144.46       organic  2018   \n",
       "0  16394524.11  12540327.19  3544729.39    309467.53  conventional  2018   \n",
       "\n",
       "             region  \n",
       "0  WestTexNewMexico  \n",
       "0  WestTexNewMexico  \n",
       "0              West  \n",
       "0              West  \n",
       "0           TotalUS  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"Date\",\"region\"], ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-04 00:00:00\n",
      "2018-03-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Date\"].min())\n",
    "print(df[\"Date\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For the avocado dataset we also have separate measurements based off of region, as can be seen above. Additionally, we have measurements taken on the same day for different types of avocado. Thus we have separate measurements for 'region' and 'type' in the dataset, subdividing it into multiple time series.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#taken from demo 20\n",
    "def plot_time_spacing_distribution(df, feature, region):\n",
    "    \"\"\"\n",
    "    Plots the distribution of time spacing for a given region.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'Location' and 'Date'.\n",
    "        region (str): The region (e.g., location) to analyze.\n",
    "    \"\"\"\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Filter data for the given region\n",
    "    region_data = df[df[feature] == region]\n",
    "    \n",
    "    if region_data.empty:\n",
    "        print(f\"No data available for region: {region}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diffs = region_data['Date'].sort_values().diff().dropna()\n",
    "    \n",
    "    # Count the frequency of each time difference\n",
    "    value_counts = time_diffs.value_counts().sort_index()\n",
    "    \n",
    "    # Display value counts\n",
    "    print(f\"Time spacing counts for {region}:\\n{value_counts}\\n\")\n",
    "    \n",
    "    # # Plot the bar chart\n",
    "    # plt.bar(value_counts.index.astype(str), value_counts.values, color='skyblue', edgecolor='black')\n",
    "    # plt.title(f\"Time Difference Distribution for {region}\")\n",
    "    # plt.xlabel(\"Time Difference (days)\")\n",
    "    # plt.ylabel(\"Frequency\")\n",
    "    # plt.xticks(rotation=45)\n",
    "    # plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spacing counts for Albany:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Atlanta:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for BaltimoreWashington:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Boise:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Boston:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for BuffaloRochester:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for California:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Charlotte:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Chicago:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for CincinnatiDayton:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Columbus:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for DallasFtWorth:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Denver:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Detroit:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for GrandRapids:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for GreatLakes:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for HarrisburgScranton:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for HartfordSpringfield:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Houston:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Indianapolis:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Jacksonville:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for LasVegas:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for LosAngeles:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Louisville:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for MiamiFtLauderdale:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Midsouth:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Nashville:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for NewOrleansMobile:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for NewYork:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Northeast:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for NorthernNewEngland:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Orlando:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Philadelphia:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for PhoenixTucson:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Pittsburgh:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Plains:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Portland:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for RaleighGreensboro:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for RichmondNorfolk:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Roanoke:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Sacramento:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for SanDiego:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for SanFrancisco:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Seattle:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for SouthCarolina:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for SouthCentral:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Southeast:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Spokane:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for StLouis:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Syracuse:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for Tampa:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for TotalUS:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for West:\n",
      "Date\n",
      "0 days    169\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for WestTexNewMexico:\n",
      "Date\n",
      "0 days    166\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_regions = df['region'].unique()\n",
    "unique_regions.tolist()\n",
    "\n",
    "for region in unique_regions:\n",
    "    plot_time_spacing_distribution(df, 'region', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spacing counts for conventional:\n",
      "Date\n",
      "0 days    8957\n",
      "7 days     168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for organic:\n",
      "Date\n",
      "0 days    8954\n",
      "7 days     168\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avocado_types = df['type'].unique()\n",
    "avocado_types.tolist()\n",
    "\n",
    "for avocado_type in avocado_types:\n",
    "    plot_time_spacing_distribution(df, 'type', avocado_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Taking a quick look at the time series distribution for each region and type, it seems like the data is equally distributed between the categorical variables identified in 1.1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There's definitely some overlap. Looking at the region names in 1.2 you can see that some of them are for specific regions such as WestTexNewMexico, while others are for much more general regions such as West or even TotalUS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from [Lecture 19](https://github.com/UBC-CS/cpsc330-2023W1/tree/main/lectures), with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag, groupby, new_feature_name=None, clip=False):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "    \n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "    \n",
    "    new_df = df.assign(**{new_feature_name : np.nan})\n",
    "    for name, group in new_df.groupby(groupby):        \n",
    "        if lag < 0: # take values from the past\n",
    "            new_df.loc[group.index[-lag:],new_feature_name] = group.iloc[:lag][orig_feature].values\n",
    "        else:       # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][orig_feature].values\n",
    "            \n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "18248 2018-03-25          1.62      15303.40  2325.30   2171.66    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "18248    10806.44    10569.80      236.64          0.0       organic  2018   \n",
       "\n",
       "                 region  \n",
       "0                Albany  \n",
       "1                Albany  \n",
       "2                Albany  \n",
       "3                Albany  \n",
       "4                Albany  \n",
       "...                 ...  \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18243 2018-02-18          1.56      17597.12  1892.05   1928.36    0.00   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18243    13776.71    13553.53      223.18          0.0       organic  2018   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0                Albany                  1.24  \n",
       "1                Albany                  1.17  \n",
       "2                Albany                  1.06  \n",
       "3                Albany                  0.99  \n",
       "4                Albany                  0.99  \n",
       "...                 ...                   ...  \n",
       "18243  WestTexNewMexico                  1.57  \n",
       "18244  WestTexNewMexico                  1.54  \n",
       "18245  WestTexNewMexico                  1.56  \n",
       "18246  WestTexNewMexico                  1.56  \n",
       "18247  WestTexNewMexico                  1.62  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 `AveragePrice` baseline \n",
    "rubric={points}\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.8285800937261841\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_train is your DataFrame\n",
    "X = df_train[[\"AveragePrice\"]].values  # Features: Just AveragePrice (n_samples, 1)\n",
    "y = df_train[\"AveragePriceNextWeek\"].values  # Target: AveragePriceNextWeek (n_samples,)\n",
    "\n",
    "# Predictions: Using AveragePrice as the constant prediction for each row\n",
    "y_pred = df_train[\"AveragePrice\"].values  # Predicted value for each row is just AveragePrice\n",
    "\n",
    "# Calculate R² score for the train data\n",
    "train_r2 = r2_score(y, y_pred)\n",
    "print(\"R² Score:\", train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² Score: 0.7631780188583048\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_test is your DataFrame\n",
    "X_test = df_test[[\"AveragePrice\"]].values  # Features: Just AveragePrice (n_samples, 1)\n",
    "y_test = df_test[\"AveragePriceNextWeek\"].values  # Target: AveragePriceNextWeek (n_samples,)\n",
    "\n",
    "# Predictions: Using AveragePrice as the constant prediction for each row\n",
    "y_pred_test = df_test[\"AveragePrice\"].values  # Predicted value for each row is just AveragePrice\n",
    "\n",
    "# Calculate R² score for the test data\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"Test R² Score:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert not train_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert not test_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert sha1(str(round(train_r2, 3)).encode('utf8')).hexdigest() == 'b1136fe2a8918904393ab6f40bfb3f38eac5fc39', \"Your training score is not correct. Are you using the right features?\"\n",
    "assert sha1(str(round(test_r2, 3)).encode('utf8')).hexdigest() == 'cc24d9a9b567b491a56b42f7adc582f2eefa5907', \"Your test score is not correct. Are you using the right features?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "rubric={points:10}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. I got to 0.80, but not beyond that. Let me know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm planning on encoding the date as a polynomial feature crossing month with day of the week. Year is already captured as a separate categorical column so I'm not planning on extracting it. I picked these two values for the data-time index because I thought they have the greatest chance of capturing a pattern in the data, as monthly changes greatly affect growth rates, and shipments usually come on certain days of the week, possibly affecting the cost of the avocado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayden\\AppData\\Local\\Temp\\ipykernel_3040\\2981256450.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['month'] = df_train['Date'].dt.month\n",
      "C:\\Users\\ayden\\AppData\\Local\\Temp\\ipykernel_3040\\2981256450.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['day_of_week'] = df_train['Date'].dt.dayofweek\n",
      "C:\\Users\\ayden\\AppData\\Local\\Temp\\ipykernel_3040\\2981256450.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['day_of_week'] = df_test['Date'].dt.dayofweek\n",
      "C:\\Users\\ayden\\AppData\\Local\\Temp\\ipykernel_3040\\2981256450.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['month'] = df_test['Date'].dt.month\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract month and day of the week\n",
    "df_train['month'] = df_train['Date'].dt.month\n",
    "df_train['day_of_week'] = df_train['Date'].dt.dayofweek\n",
    "\n",
    "df_test['day_of_week'] = df_test['Date'].dt.dayofweek\n",
    "df_test['month'] = df_test['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'll start with organizing my features\n",
    "\n",
    "numeric_features = [\"Total Volume\", \"4046\", \"4225\", \"4770\", \"Total Bags\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\", \"AveragePrice\"]\n",
    "categorical_features = [\"type\", \"year\", \"region\"]\n",
    "target = [\"AveragePriceNextWeek\"]\n",
    "drop_features = [\"Date\"]\n",
    "\n",
    "polynomial_features = ['month', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# make a transformer for each type of feature, adpated from demo-20\n",
    "def preprocess_features(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    polynomial_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse_output=False)\n",
    "    \n",
    "    date_poly_pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False),  # Cross terms for month and day_of_week\n",
    "    StandardScaler()  # Standardize the polynomial features\n",
    "    )\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "            (numeric_transformer, numeric_features),\n",
    "            (categorical_transformer, categorical_features),\n",
    "            (date_poly_pipeline, polynomial_features),\n",
    "            (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(df_train)\n",
    "    encoder = preprocessor.named_transformers_['onehotencoder']\n",
    "    #print(\"Categories seen during training:\", encoder.categories_)\n",
    "    #print(\"New categories during transform:\", df_test['region'].unique())\n",
    "    #print(df_train[['type', 'year', 'region']].dtypes)\n",
    "\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_['onehotencoder'].get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    poly_feature_names = (\n",
    "        preprocessor.named_transformers_['pipeline'].named_steps['polynomialfeatures'].get_feature_names_out(polynomial_features)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    new_columns = numeric_features + ohe_feature_names + poly_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(df_train), index=df_train.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(df_test), index=df_test.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = df_train[target]\n",
    "    y_test = df_test[target]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayden\\miniconda3\\envs\\cpsc330\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    polynomial_features,\n",
    "    drop_features, target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>type_organic</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Syracuse</th>\n",
       "      <th>region_Tampa</th>\n",
       "      <th>region_TotalUS</th>\n",
       "      <th>region_West</th>\n",
       "      <th>region_WestTexNewMexico</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month^2</th>\n",
       "      <th>month day_of_week</th>\n",
       "      <th>day_of_week^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.234535</td>\n",
       "      <td>-0.229503</td>\n",
       "      <td>-0.222203</td>\n",
       "      <td>-0.214954</td>\n",
       "      <td>-0.232206</td>\n",
       "      <td>-0.229907</td>\n",
       "      <td>-0.223154</td>\n",
       "      <td>-0.172063</td>\n",
       "      <td>-0.432512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.094284</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.234440</td>\n",
       "      <td>-0.230948</td>\n",
       "      <td>-0.219448</td>\n",
       "      <td>-0.214272</td>\n",
       "      <td>-0.233587</td>\n",
       "      <td>-0.231513</td>\n",
       "      <td>-0.223789</td>\n",
       "      <td>-0.172063</td>\n",
       "      <td>-0.383676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.094284</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.233469</td>\n",
       "      <td>-0.231018</td>\n",
       "      <td>-0.219530</td>\n",
       "      <td>-0.214196</td>\n",
       "      <td>-0.229850</td>\n",
       "      <td>-0.226469</td>\n",
       "      <td>-0.224325</td>\n",
       "      <td>-0.172063</td>\n",
       "      <td>-0.554604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.094284</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.233283</td>\n",
       "      <td>-0.230996</td>\n",
       "      <td>-0.218170</td>\n",
       "      <td>-0.213945</td>\n",
       "      <td>-0.230999</td>\n",
       "      <td>-0.228629</td>\n",
       "      <td>-0.222193</td>\n",
       "      <td>-0.172063</td>\n",
       "      <td>-0.823205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.094284</td>\n",
       "      <td>-1.532848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.225747</td>\n",
       "      <td>-0.230668</td>\n",
       "      <td>-0.196131</td>\n",
       "      <td>-0.213811</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>-0.229930</td>\n",
       "      <td>-0.224856</td>\n",
       "      <td>-0.172063</td>\n",
       "      <td>-0.994133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.229648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.023758</td>\n",
       "      <td>-1.229648</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Volume      4046      4225      4770  Total Bags  Small Bags  \\\n",
       "0     -0.234535 -0.229503 -0.222203 -0.214954   -0.232206   -0.229907   \n",
       "1     -0.234440 -0.230948 -0.219448 -0.214272   -0.233587   -0.231513   \n",
       "2     -0.233469 -0.231018 -0.219530 -0.214196   -0.229850   -0.226469   \n",
       "3     -0.233283 -0.230996 -0.218170 -0.213945   -0.230999   -0.228629   \n",
       "4     -0.225747 -0.230668 -0.196131 -0.213811   -0.232627   -0.229930   \n",
       "\n",
       "   Large Bags  XLarge Bags  AveragePrice  type_organic  ...  region_Syracuse  \\\n",
       "0   -0.223154    -0.172063     -0.432512           0.0  ...              0.0   \n",
       "1   -0.223789    -0.172063     -0.383676           0.0  ...              0.0   \n",
       "2   -0.224325    -0.172063     -0.554604           0.0  ...              0.0   \n",
       "3   -0.222193    -0.172063     -0.823205           0.0  ...              0.0   \n",
       "4   -0.224856    -0.172063     -0.994133           0.0  ...              0.0   \n",
       "\n",
       "   region_Tampa  region_TotalUS  region_West  region_WestTexNewMexico  \\\n",
       "0           0.0             0.0          0.0                      0.0   \n",
       "1           0.0             0.0          0.0                      0.0   \n",
       "2           0.0             0.0          0.0                      0.0   \n",
       "3           0.0             0.0          0.0                      0.0   \n",
       "4           0.0             0.0          0.0                      0.0   \n",
       "\n",
       "      month  day_of_week   month^2  month day_of_week  day_of_week^2  \n",
       "0 -1.532848          0.0 -1.094284          -1.532848            0.0  \n",
       "1 -1.532848          0.0 -1.094284          -1.532848            0.0  \n",
       "2 -1.532848          0.0 -1.094284          -1.532848            0.0  \n",
       "3 -1.532848          0.0 -1.094284          -1.532848            0.0  \n",
       "4 -1.229648          0.0 -1.023758          -1.229648            0.0  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def score_lr_print_coeff(preprocessor, df_train, y_train, df_test, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, Ridge())\n",
    "    lr_pipe.fit(df_train, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(df_train, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(df_test, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"ridge\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayden\\miniconda3\\envs\\cpsc330\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.315157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_organic</th>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.101904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.099640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.077909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.053540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month^2</th>\n",
       "      <td>-0.073466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.073669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.076250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.087666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.315157\n",
       "type_organic                0.115813\n",
       "region_SanFrancisco         0.101904\n",
       "region_HartfordSpringfield  0.099640\n",
       "region_NewYork              0.077909\n",
       "...                              ...\n",
       "region_Denver              -0.053540\n",
       "month^2                    -0.073466\n",
       "region_SouthCentral        -0.073669\n",
       "region_DallasFtWorth       -0.076250\n",
       "region_Houston             -0.087666\n",
       "\n",
       "[72 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_lr_print_coeff(preprocessor, df_train, y_train, df_test, y_test, X_train_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using Ridge, I achieved a test score of 0.80 which is pretty decent. Looking at the coefficients, we can see some helpful results. AveragePrice has the most positive influence on AveragePriceNextWeek (unsurprisingly), but organic has the second highest influence (which is also not the surprising). Returning to the research question that prompted the analysis of this dataset, San Francisco seems to be the region with the largest positive correlation on average Avocado prices, and Houston seems to the region with the smallest. Several other regions in Texas also have a negative correlation, so if you want cheap avocados, live in Texas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 20 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer.\n",
    "3. When studying time series modeling, we explored several ways to encode date information as a feature for the citibike dataset. When we used time of day as a numeric feature, the Ridge model was not able to capture the periodic pattern. Why? How did we tackle this problem? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Whale sightings off the coast of California.\n",
    "2. Creating lagged versions of these features would struggle. If the time is unevenly spaced, the lagged features would also be unevenly spaced. TwoDaysAhead versus OneWeekAhead could have a very different effect on the target value and so you would have to make multiple lagged features for each time disparity.\n",
    "3. This is because a cyclic pattern like time-of-day for timeseries is inherently non-linear, so Ridge was no able to tackle the periodic pattern. To address this problem, we encoded time-of-day as a categorical feature with one-hot-encoder which created better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Computer vision \n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 19 on multiclass classification and introduction to computer vision. \n",
    "\n",
    "1. How many parameters (coefficients and intercepts) will `sklearn`’s `LogisticRegression()` model learn for a four-class classification problem, assuming that you have 10 features? Briefly explain your answer.\n",
    "2. In Lecture 19, we briefly discussed how neural networks are sort of like `sklearn`'s pipelines, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?\n",
    "3. Imagine that you have a small dataset with ~1000 images containing pictures and names of 50 different Computer Science faculty members from UBC. Your goal is to develop a reasonably accurate multi-class classification model for this task. Describe which model/technique you would use and briefly justify your choice in one to three sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For multi-class problems, LogisticRegression learns a coefficient for each feature for each class. It also learns an intercept for each class. Therefore the model would have 44 parameters.\n",
    "2. This is useful because you can start with a robust model and then build off it to tailor it to your question as opposed to doing everything by scratch. This is because tailoring an existing model is simply involves carrying out more sequential transformations.\n",
    "3. I would use an existing model like Inception and then build on it using transfer learning to tailor it towards our dataset. Because the general datasets like ImageNet that most of these models are trained on probably wont have the classes we're looking for (individual pictures of CS professors), we have to train them on our own dataset. The different classes would presumably be each of the different faculty members and being able to identify their faces and assign them to the right name (class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "name": "_merged",
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
